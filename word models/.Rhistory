stri_join_list(sep = ' ') |>
str_split('') |>
unlist() |>
duplicate_mid() |>
matrix(ncol = 2, byrow = T)
txt_mat
marginal_data <- txt_word |>
as.list() |>
stri_join_list(sep = ' ') |>
str_split('') |>
unlist() |>
matrix(ncol = 1) |>
data.frame() |>
setNames('letter') |>
group_by(letter) |>
count() |>
ungroup() |>
filter(letter != ' ') |>
mutate(n = summalize(n)) |>
setNames(c('letter', 'Probability'))
p1 <- marginal_data |>
ggplot()+
geom_col(aes(x = letter, y = Probability),
col = 'white', fill = 'blue',alpha = .5) +
labs(title = 'Marginal probability: P(Letter)',
x = 'Letter', y = 'Probability')+
theme_classic()+
coord_flip()
p1
joint_data <- txt_mat |>
data.frame() |>
setNames(c('now', 'nxt')) |>
dplyr::filter(now != ' ' & nxt != ' ') |>
group_by(now, nxt) |>
count() |>
ungroup() |>
mutate(n = summalize(n)) |>
setNames(c('now', 'nxt', 'Probability'))
View(joint_data)
p2 <- joint_data |>
ggplot(col = 'white')+
geom_tile(aes(x = now, y = nxt, fill = Probability),col = 'white')+
labs(title = 'Joint Probability: P(Current, Next)',
x = 'Current letter',
y = 'Next letter')+
theme_classic()
p2
features <- c(letters)
for (i in 1:length(features))
{
if (i == 1)
{
cc_data <- txt_mat |>
data.frame() |>
setNames(c('now', 'nxt')) |>
filter(now != ' ' & nxt != ' ') |> # omitting spaces
filter(now == features[i]) |>
group_by(now, nxt) |>
count() |>
ungroup() |>
mutate(n = summalize(n))
cn_data <- txt_mat |>
data.frame() |>
setNames(c('now', 'nxt')) |>
filter(now != ' ' & nxt != ' ') |> # omitting spaces
filter(nxt == features[i]) |>
group_by(now, nxt) |>
count() |>
ungroup() |>
mutate(n = summalize(n))
}
else
{
temp_data1 <- txt_mat |>
data.frame() |>
setNames(c('now', 'nxt')) |>
filter(now != ' ' & nxt != ' ') |> # omitting spaces
filter(now == features[i]) |>
group_by(now, nxt) |>
count() |>
ungroup() |>
mutate(n = summalize(n))
cc_data <- rbind(cc_data, temp_data1)
temp_data2 <- txt_mat |>
data.frame() |>
setNames(c('now', 'nxt')) |>
filter(now != ' ' & nxt != ' ') |> # omitting spaces
filter(nxt == features[i]) |>
group_by(now, nxt) |>
count() |>
ungroup() |>
mutate(n = summalize(n))
cn_data <- rbind(cn_data, temp_data2)
}
cc_data <- cc_data |>
setNames(c('now', 'nxt', 'Probability'))
cn_data <- cn_data |>
setNames(c('now', 'nxt', 'Probability'))
View(cc_data)
View(cn_data)
p3 <- cc_data |>
ggplot()+
geom_tile(aes(x = now, y = nxt, fill = Probability), col = 'white')+
labs(title = 'Conditional probability: P(Next | Current = *)',
x = 'Current letter',
y = 'Next letter')+
theme_classic()
p3
p4 <- cn_data |>
ggplot()+
geom_tile(aes(x = now, y = nxt, fill = Probability), col = 'white')+
labs(title = 'Conditional probability: P(Current | Next = *)',
x = 'Current letter',
y = 'Next letter')+
theme_classic()
p4
gridExtra::grid.arrange(p1, p2, p3, p4,
nrow = 2)
# distribution of letters which most commonly start a word
starter <- txt_mat |>
data.frame() |>
setNames(c('now', 'nxt')) |>
filter(now == ' ') |>
group_by(now, nxt) |>
count() |>
ungroup() |>
mutate(n = summalize(n)) |>
setNames(c('now', 'nxt', 'Probability')) |>
arrange(desc(Probability))
finalizer <- txt_mat |>
data.frame() |>
setNames(c('now', 'nxt')) |>
filter(nxt == ' ') |>
group_by(now, nxt) |>
count() |>
ungroup() |>
mutate(n = summalize(n)) |>
setNames(c('now', 'nxt', 'Probability')) |>
arrange(desc(Probability))
gridExtra::grid.arrange(
starter |>
ggplot()+
geom_col(aes(x = reorder(nxt, Probability), y = Probability))+
labs(title = 'Distribution of word starters',
x = 'Letter', y = 'Probability of starting an english word')+
coord_flip()+
theme_classic(),
finalizer |>
ggplot()+
geom_col(aes(x = reorder(now, Probability), y = Probability))+
labs(title = 'Distribution of word enders',
x = '', y = 'Probability of ending an english word')+
coord_flip()+
theme_classic(),
nrow = 1
)
start_end <- merge(starter, finalizer,
by.x = 'nxt', by.y = 'now') |>
select(!c('now', 'nxt.y')) |>
setNames(c('letter', 'prob.start', 'prob.end')) |>
reshape2::melt(id = 'letter')
# How many times does a letter appear in a word on average
colMedians <- function(x) apply(x, 2, median)
colMax <- function(x) apply(x, 2, max)
colMin <- function(x) apply(x, 2, min)
vowels <- c('a','e', 'i', 'o', 'u')
consonants <- letters[!letters %in% vowels]
freq_occ <- max_possible <- min_possible <- vector()
for (i in 1:length(letters))
{
freq_occ[i] <- txt_word |>
mutate(count = str_count(word, pattern = letters[i])) |>
mutate(count = ifelse(count == 0, NA, count)) |>
na.omit() |>
select(count) |>
colMedians()
max_possible[i] <- txt_word |>
mutate(count = str_count(word, pattern = letters[i])) |>
select(count) |>
colMax()
min_possible[i] <- txt_word |>
mutate(count = str_count(word, pattern = letters[i])) |>
select(count) |>
colMin()
}
freq_occ <- max_possible <- min_possible <- vector()
for (i in 1:length(letters))
{
freq_occ[i] <- txt_word |>
mutate(count = str_count(word, pattern = letters[i])) |>
mutate(count = ifelse(count == 0, NA, count)) |>
na.omit() |>
dplyr::select(count) |>
colMedians()
max_possible[i] <- txt_word |>
mutate(count = str_count(word, pattern = letters[i])) |>
dplyr::select(count) |>
colMax()
min_possible[i] <- txt_word |>
mutate(count = str_count(word, pattern = letters[i])) |>
dplyr::select(count) |>
colMin()
}
s()
ls()
save.image("~/Documents/Text processing NLP/Letter models/march 10 2023.RData")
setwd("~/Documents/Text processing NLP/word models")
ls()
starter
head(txt_mat, 20)
# distribution of letters which most commonly start a word
starter <- txt_mat |>
data.frame() |>
setNames(c('now', 'nxt')) |>
filter(now == ' ') |>
group_by(nxt) |>
count() |>
ungroup() |>
mutate(n = summalize(n)) |>
setNames(c('now', 'nxt', 'Probability')) |>
arrange(desc(Probability))
txt_mat |>
data.frame() |>
setNames(c('now', 'nxt')) |>
filter(now == ' ') |>
group_by(nxt) |>
count()
# distribution of letters which most commonly start a word
starter <- txt_mat |>
data.frame() |>
setNames(c('now', 'nxt')) |>
filter(now == ' ') |>
group_by(nxt) |>
count() |>
ungroup() |>
mutate(n = summalize(n)) |>
setNames(c('nxt', 'Probability')) |>
arrange(desc(Probability))
starter
starter <- txt_mat |>
data.frame() |>
setNames(c('now', 'nxt')) |>
filter(now == ' ') |>
group_by(nxt) |>
count() |>
ungroup() |>
mutate(n = summalize(n)) |>
setNames(c('nxt', 'Probability')) |>
arrange(desc(Probability))
starter
xx = sample(x = starter[, 'nxt'] |> unlist() |> unname(),
size = 1000, replace = T,
prob = starter[, 'Probability'] |> unlist() |> unname())
table(xx)
(w.l <- rnorm(n = 10, mean = model_gaussian$estimate[1], sd = model_gaussian$estimate[2]) |> trunc())
model_pois <- fitdist(txt_len$num, distr = 'pois')
rm(list = ls())
# libs
require(pacman)
p_load(gtools, tidytext, janeaustenr, stringr, stringi, dplyr,
ggplot2, ggpubr, gridExtra, fitdistrplus)
# loading data into my environment
load("~/Documents/Text processing NLP/word models/march 10 2023.RData")
txt_len <- txt_word |>
mutate(num = str_length(word))
sumtbl <- txt_len |>
filter(num < 22) |>
summarise(
Min = min(num),
`Q1 (25%)` = quantile(num, .25),
Mean = mean(num),
Median = median(num),
`Q3 (75%)` = quantile(num, .75),
Max = max(num),
Skewness = DescTools::Skew(num),
Kurtosis = DescTools::Kurt(num)
)
t1 <- ggtexttable(round(sumtbl, 2), rows = NULL)
t2 <- txt_len |>
filter(num < 22) |>
ggplot()+
geom_bar(aes(x =factor (num)))+
labs(title = 'Word length distribution',
x = 'Word length', y = 'Count')+
theme_classic()
ggarrange(t2, t1, nrow = 2,heights = c(4,1))
# fitting the binomial model
model_pois <- fitdist(txt_len$num, distr = 'pois')
model_gaussian <- fitdist(txt_len$num, distr = 'norm')
model_pois
model_gaussian
sd(txt_len$num)
dim(txt_len)
(w.l <- rnorm(n = 10, mean = model_gaussian$estimate[1], sd = model_gaussian$estimate[2]) |> trunc())
# Simple generation without looking at joint probs
for (wl in 1:length(w.l))
{
# the starting letter
start <- sample(x = starter[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = starter[, 'Probability'] |> unlist() |> unname())
wrd <- start
for (ch in 2:w.l[wl])
{
nxt_u <- cc_data |>
filter(now == wrd[ch - 1])
# finding the next letter
nxt_letter <- sample(x = nxt_u[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = nxt_u[, 'Probability'] |> unlist() |> unname())
wrd <- c(wrd, nxt_letter)
}
wrd <- list(wrd) |> stri_join_list()
print(wrd)
}
# loading data into my environment
load("~/Documents/Text processing NLP/word models/march 10 2023.RData")
txt_len <- txt_word |>
mutate(num = str_length(word))
sumtbl <- txt_len |>
filter(num < 22) |>
summarise(
Min = min(num),
`Q1 (25%)` = quantile(num, .25),
Mean = mean(num),
Median = median(num),
`Q3 (75%)` = quantile(num, .75),
Max = max(num),
Skewness = DescTools::Skew(num),
Kurtosis = DescTools::Kurt(num)
)
t1 <- ggtexttable(round(sumtbl, 2), rows = NULL)
t2 <- txt_len |>
filter(num < 22) |>
ggplot()+
geom_bar(aes(x =factor (num)))+
labs(title = 'Word length distribution',
x = 'Word length', y = 'Count')+
theme_classic()
# fitting the binomial model
model_pois <- fitdist(txt_len$num, distr = 'pois')
model_gaussian <- fitdist(txt_len$num, distr = 'norm')
(w.l <- rnorm(n = 10, mean = model_gaussian$estimate[1], sd = model_gaussian$estimate[2]) |> trunc())
# Simple generation without looking at joint probs
for (wl in 1:length(w.l))
{
# the starting letter
start <- sample(x = starter[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = starter[, 'Probability'] |> unlist() |> unname())
wrd <- start
for (ch in 2:w.l[wl])
{
nxt_u <- cc_data |>
filter(now == wrd[ch - 1])
# finding the next letter
nxt_letter <- sample(x = nxt_u[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = nxt_u[, 'Probability'] |> unlist() |> unname())
wrd <- c(wrd, nxt_letter)
}
wrd <- list(wrd) |> stri_join_list()
print(wrd)
}
(w.l <- rnorm(n = 10, mean = model_gaussian$estimate[1], sd = model_gaussian$estimate[2]) |> trunc())
# Simple generation without looking at joint probs
for (wl in 1:length(w.l))
{
# the starting letter
start <- sample(x = starter[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = starter[, 'Probability'] |> unlist() |> unname())
wrd <- start
for (ch in 2:w.l[wl])
{
nxt_u <- cc_data |>
filter(now == wrd[ch - 1])
# finding the next letter
nxt_letter <- sample(x = nxt_u[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = nxt_u[, 'Probability'] |> unlist() |> unname())
wrd <- c(wrd, nxt_letter)
}
wrd <- list(wrd) |> stri_join_list()
print(wrd)
}
(w.l <- rnorm(n = 10, mean = model_gaussian$estimate[1], sd = model_gaussian$estimate[2]) |> trunc())
# Simple generation without looking at joint probs
for (wl in 1:length(w.l))
{
# the starting letter
start <- sample(x = starter[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = starter[, 'Probability'] |> unlist() |> unname())
wrd <- start
for (ch in 2:w.l[wl])
{
nxt_u <- cc_data |>
filter(now == wrd[ch - 1])
# finding the next letter
nxt_letter <- sample(x = nxt_u[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = nxt_u[, 'Probability'] |> unlist() |> unname())
wrd <- c(wrd, nxt_letter)
}
wrd <- list(wrd) |> stri_join_list()
print(wrd)
}
(w.l <- rnorm(n = 10, mean = model_gaussian$estimate[1], sd = model_gaussian$estimate[2]) |> trunc())
# Simple generation without looking at joint probs
for (wl in 1:length(w.l))
{
# the starting letter
start <- sample(x = starter[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = starter[, 'Probability'] |> unlist() |> unname())
wrd <- start
for (ch in 2:w.l[wl])
{
nxt_u <- cc_data |>
filter(now == wrd[ch - 1])
# finding the next letter
nxt_letter <- sample(x = nxt_u[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = nxt_u[, 'Probability'] |> unlist() |> unname())
wrd <- c(wrd, nxt_letter)
}
wrd <- list(wrd) |> stri_join_list()
print(wrd)
}
(w.l <- rnorm(n = 10, mean = model_gaussian$estimate[1], sd = model_gaussian$estimate[2]) |> trunc())
# Simple generation without looking at joint probs
for (wl in 1:length(w.l))
{
# the starting letter
start <- sample(x = starter[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = starter[, 'Probability'] |> unlist() |> unname())
wrd <- start
for (ch in 2:w.l[wl])
{
nxt_u <- cc_data |>
filter(now == wrd[ch - 1])
# finding the next letter
nxt_letter <- sample(x = nxt_u[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = nxt_u[, 'Probability'] |> unlist() |> unname())
wrd <- c(wrd, nxt_letter)
}
wrd <- list(wrd) |> stri_join_list()
print(wrd)
}
w.l
w.l[w.l > 1]
(w.l <- rnorm(n = 10, mean = model_gaussian$estimate[1], sd = model_gaussian$estimate[2]) |> trunc())
w.l <- w.l[w.l > 1]
# Simple generation without looking at joint probs
for (wl in 1:length(w.l))
{
# the starting letter
start <- sample(x = starter[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = starter[, 'Probability'] |> unlist() |> unname())
wrd <- start
for (ch in 2:w.l[wl])
{
nxt_u <- cc_data |>
filter(now == wrd[ch - 1])
# finding the next letter
nxt_letter <- sample(x = nxt_u[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = nxt_u[, 'Probability'] |> unlist() |> unname())
wrd <- c(wrd, nxt_letter)
}
wrd <- list(wrd) |> stri_join_list()
print(wrd)
}
# Generation of text paying attention to joint probs
for (wl in 1:length(w.l))
{
# the starting letter
start <- sample(x = starter[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = starter[, 'Probability'] |> unlist() |> unname())
wrd <- start
for (ch in 2:w.l[wl])
{
nxt_u <- cc_data |>
filter(now == wrd[ch - 1])
# a probability of acceptance based on joint probs
u <- 1.529e-03
j.prob <- 0
i = 0
while (j.prob < u)
{
j.prob <- joint_data |>
filter(nxt == nxt_letter, now == wrd[ch - 1]) |>
dplyr::select(Probability) |>
as.numeric()
# finding the next letter
nxt_letter <- sample(x = nxt_u[, 'nxt'] |> unlist() |> unname(),
size = 1,
prob = nxt_u[, 'Probability'] |> unlist() |> unname())
# condition for terminating words taking more than 10 iterations to build
i = i + 1
if (i > 10) {message('broken'); break}
}
wrd <- c(wrd, nxt_letter)
}
wrd <- list(wrd) |> stri_join_list()
print(wrd)
}
